{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627f0e55-6d36-456b-8832-21a4bcff4cbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nonbinary classification neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0d425d8c-9f16-4839-b5d4-6296c11a7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import requests\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch import tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from fastbook import show_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d129b08-64a6-4146-a45e-3ac4172d359d",
   "metadata": {},
   "source": [
    "#### MetaParameters\n",
    "- keeping track of all global variables: Batch size, learning rate, number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d3938e-009c-4bad-bc64-d786a3cebec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaParameters(batch_size=64, learning_rate=1.0, n_epochs=8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ADJUST METAPARAMETERS HERE\n",
    "@dataclass\n",
    "class MetaParameters:\n",
    "    batch_size: int\n",
    "    learning_rate: float\n",
    "    n_epochs: int\n",
    "    \n",
    "mp = MetaParameters(64, 1., 8)\n",
    "mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d96152bb-f6f0-4497-b794-dc8ab1e30dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda')\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80b04f7c-96cc-4168-8f1c-60f3e21c8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(path: Path):\n",
    "    return [p for p in path.glob('./*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "49e8b620-1545-46e4-997f-d3b5c91431eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/c/projects/NN-from-scratch')\n",
    "dls = {\n",
    "    'train': DataLoader(datasets.MNIST(path, download=True, train=True, transform=transforms.ToTensor()),\n",
    "                        batch_size=mp.batch_size,\n",
    "                        shuffle=True),\n",
    "    'valid': DataLoader(datasets.MNIST(path, download=True, train=False, transform=transforms.ToTensor()),\n",
    "                        batch_size=mp.batch_size,\n",
    "                        shuffle=True),\n",
    "}\n",
    "#https://blog.paperspace.com/dataloaders-abstractions-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c0cfc4ba-cafd-492c-83a4-7ed1a1ec80d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dls['train']))\n",
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9cbe4-0db9-4c38-8596-54c2793bf660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98fefcd5-f6a0-4dff-b80a-48e32e4714f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets) -> float:\n",
    "    return -(predictions.softmax(-1)[targets].log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fa1734bd-796e-43fd-8de8-2ebf7011545f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m targets[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m dls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x, y)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    169\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:180\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "preds = torch.rand(10) * 10\n",
    "targets = torch.zeros(10)\n",
    "targets[3] = 1\n",
    "for x, y in dls['train']:\n",
    "    print(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0d2128c-747f-482a-bf16-d85a01295408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(shape):\n",
    "    return torch.randn(shape, requires_grad=True, dtype=torch.float, device=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6c7e3-524e-4bec-8035-c812d09ccd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dl, params):\n",
    "    for x, y in dl:\n",
    "        preds = model(x, weights, bias)\n",
    "        loss = mnist_loss(preds, y)\n",
    "        loss.backward()\n",
    "        for p in params:\n",
    "            p.data -= p.grad * mp.learning_rate\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f998ce61-be43-453b-9871-5c1391885f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = init_params((28*28, 1))\n",
    "bias = init_params(1)\n",
    "preds = (torch.rand(10, 10)).softmax(-1)\n",
    "targs = (torch.rand(10)).softmax(-1)\n",
    "(preds.argmax(1)==targs).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "21ac6d41-d623-44f8-b250-1bf7bad8f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(preds, trgts):\n",
    "    preds = preds.softmax(-1)\n",
    "    return ((preds.argmax(1)) == trgts).float().mean()\n",
    "\n",
    "def accuracy(valid_dl, weights, bias):\n",
    "    accs = [batch_accuracy(model(xb, weights, bias), yb) for xb, yb in valid_dl]\n",
    "    return f'{round(torch.stack(accs).mean().item(), 3) * 100}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc291b-d6a8-48b0-b5d3-45fa2c52efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9541667\n",
      "0.962882\n",
      "0.9662269\n",
      "0.9690509\n",
      "0.969132\n",
      "0.9709144\n",
      "0.9735186\n",
      "0.9740394\n"
     ]
    }
   ],
   "source": [
    "params = weights, bias\n",
    "for i in range(mp.n_epochs):\n",
    "    #train_epoch(train_dl, model, mp.learning_rate, params)\n",
    "    #print(validate_epoch(model))\n",
    "    train(dls['train'], params)\n",
    "    print(accuracy(dls['valid'], weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f9974-2795-45dc-82ac-95fda64d0b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
